{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teo/userdata/git_libraries/Yeominrak\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from music21 import converter, stream, note as m21_note \n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from yeominrak_processing import AlignedScore, SamplingScore, pack_collate, ShiftedAlignedScore, OrchestraScore\n",
    "from model_zoo import Seq2seq, Converter, AttentionSeq2seq, QkvAttnSeq2seq, get_emb_total_size, QkvAttnSeq2seqOrch\n",
    "import random as random\n",
    "from decode import MidiDecoder, OrchestraDecoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from fractions import Fraction\n",
    "\n",
    "import math\n",
    "def round_number(number, decimals=3):\n",
    "    scale = 10.0 ** decimals\n",
    "    return math.floor(number * scale) / scale\n",
    "\n",
    "\n",
    "def prepare_input_for_next_part(outputs_tensor):\n",
    "  pred, condition = outputs_tensor[:, :3], outputs_tensor[:, 3:]\n",
    "  next_input = torch.cat([torch.cat([pred[0:1], torch.tensor([[3,3,3]])], dim=1) , torch.cat([pred[1:], condition[:-1]], dim=1) ], dim=0)\n",
    "  return next_input\n",
    "\n",
    "def get_measure_specific_output(output:torch.LongTensor, measure_idx_in_token:int):\n",
    "  corresp_ids = torch.where(output[:,-1] == measure_idx_in_token)[0]+1\n",
    "  return output[corresp_ids]\n",
    "\n",
    "\n",
    "def fix_measure_idx(sample:torch.LongTensor):\n",
    "  new_sample = sample.clone()\n",
    "  current_measure = 2\n",
    "  for note in new_sample:\n",
    "    if note[1] in (1, 2): continue\n",
    "    if note[3] == 3: # beat_start\n",
    "      current_measure += 1\n",
    "    note[5] = current_measure\n",
    "  return new_sample\n",
    "\n",
    "\n",
    "\n",
    "def fill_in_source(src, num_measure):\n",
    "  # src: list of token in strings\n",
    "  outputs = []\n",
    "  for note in src:\n",
    "    if note[-1] in range(4-num_measure):\n",
    "      continue\n",
    "    else:\n",
    "      outputs.append(note)\n",
    "  return outputs\n",
    "\n",
    "def get_measure_shifted_output(output: torch.Tensor):\n",
    "  for i, token in enumerate(output):\n",
    "    if token[-1] == 4: # new measure started:\n",
    "      break\n",
    "  for j in range(i, len(output)):\n",
    "    if output[j][-1] == 6:\n",
    "      break\n",
    "\n",
    "  output_tokens_from_second_measure = output[i:j].clone()\n",
    "  output_tokens_from_second_measure[0, 1] = 1\n",
    "  output_tokens_from_second_measure[0, 2] = 1\n",
    "  output_tokens_from_second_measure[:, 5] -= 1\n",
    "  return output_tokens_from_second_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_name = 'vanilla_CHP'\n",
    "state = torch.load('outputs/2023-12-20/16-02-36/epoch600_model.pt')\n",
    "state_alt = torch.load('outputs/2023-12-20/16-02-36/best_loss_model.pt')\n",
    "\n",
    "config = OmegaConf.load('yamls/orchestration.yaml')\n",
    "config = get_emb_total_size(config)\n",
    "\n",
    "output_dir = Path('gen_results/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = OrchestraScore(is_valid= True,\n",
    "                            # valid_measure_num = [i for i in range(93, 99)],\n",
    "                            xml_path='yeominlak_omr5.musicxml', \n",
    "                            use_pitch_modification=False, \n",
    "                            slice_measure_num=config.data.max_meas,\n",
    "                            min_meas=config.data.min_meas,\n",
    "                            feature_types=config.model.features,\n",
    "                              sampling_rate=config.data.sampling_rate)\n",
    "\n",
    "# test_set = ShiftedAlignedScore(is_valid= True,\n",
    "#                                    xml_path = 'gen_results/CHP_scoreCPH_seq2seq_grace2_errorfixed.musicxml',\n",
    "#                                     valid_measure_num = 'entire',\n",
    "#                                     slice_measure_num=config.data.max_meas,\n",
    "#                                     min_meas=config.data.min_meas,\n",
    "#                                     feature_types=config.model.features,\n",
    "#                                       sampling_rate=config.data.sampling_rate)\n",
    "\n",
    "test_set = ShiftedAlignedScore(is_valid= True,\n",
    "                                   xml_path = 'gen_results/CHP_scoreseq2seq_grace2_errorfixed.musicxml',\n",
    "                                    valid_measure_num = 'entire',\n",
    "                                    slice_measure_num=config.data.max_meas,\n",
    "                                    min_meas=config.data.min_meas,\n",
    "                                    feature_types=config.model.features,\n",
    "                                      sampling_rate=config.data.sampling_rate)\n",
    "\n",
    "\n",
    "test_set.result_pairs = [x for x in test_set.result_pairs if x[0] == 0][:len(test_set.slice_info)]\n",
    "for pair in test_set.result_pairs:\n",
    "  pair[0] = 7\n",
    "  pair[1] = 7\n",
    "test_set.tokenizer = val_dataset.era_dataset.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(30):\n",
    "  offset = round_number(i/3, 3)\n",
    "  if offset in test_set.tokenizer.tok2idx['offset']:\n",
    "     continue\n",
    "  if i < 6:\n",
    "    other_value = i % 3 + 6\n",
    "  else:\n",
    "    other_value = i % 3 + 21\n",
    "  test_set.tokenizer.tok2idx['offset'][offset] = test_set.tokenizer.tok2idx['offset'][round_number(other_value/3, 3)]\n",
    "\n",
    "for i in range(30):\n",
    "  offset = round_number(i/2, 3)\n",
    "  if offset in test_set.tokenizer.tok2idx['offset']:\n",
    "     continue\n",
    "  other_value = i - 2\n",
    "  test_set.tokenizer.tok2idx['offset'][offset] = test_set.tokenizer.tok2idx['offset'][round_number(other_value/2, 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('outputs/2023-12-20/16-02-36/epoch400_model.pt')\n",
    "state_alt = torch.load('outputs/2023-12-20/16-02-36/epoch200_model.pt')\n",
    "\n",
    "model = QkvAttnSeq2seqOrch(val_dataset.era_dataset.tokenizer, val_dataset.tokenizer, config.model).to(device)\n",
    "# state = torch.load('best_model.pt')\n",
    "model.is_condition_shifted = True\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "model_alt = QkvAttnSeq2seqOrch(val_dataset.era_dataset.tokenizer, val_dataset.tokenizer, config.model).to(device)\n",
    "model_alt.is_condition_shifted = True\n",
    "model_alt.load_state_dict(state_alt)\n",
    "model_alt.eval()\n",
    "\n",
    "decoder = OrchestraDecoder(val_dataset.tokenizer)\n",
    "source_decoder = MidiDecoder(val_dataset.era_dataset.tokenizer)\n",
    "\n",
    "\n",
    "if 'offset' in model.tokenizer.tok2idx:\n",
    "  for i in range(120):\n",
    "    if i % 4 == 0:\n",
    "      continue\n",
    "    if i / 4 not in model.tokenizer.tok2idx['offset']:\n",
    "      model.tokenizer.tok2idx['offset'][i/4] = model.tokenizer.tok2idx['offset'][(i-2)/4]\n",
    "\n",
    "model_alt.tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de08a31aab7047fbb48f567a35fc37c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 32.25, inst: 0, measure: 29\n",
      "duration error: 30.25, inst: 0, measure: 31\n",
      "duration error: 35.25, inst: 0, measure: 44\n",
      "duration error: 30.25, inst: 0, measure: 50\n",
      "duration error: 31.0, inst: 0, measure: 60\n",
      "duration error: 34.75, inst: 0, measure: 126\n",
      "duration error: 34.75, inst: 0, measure: 140\n",
      "duration error: 34.0, inst: 0, measure: 145\n",
      "duration error: 34.75, inst: 0, measure: 159\n",
      "duration error: 34.75, inst: 0, measure: 161\n",
      "duration error: 31.75, inst: 0, measure: 179\n",
      "duration error: 33.75, inst: 0, measure: 182\n",
      "duration error: 30.5, inst: 0, measure: 194\n",
      "duration error: 31.75, inst: 0, measure: 196\n",
      "duration error: 35.25, inst: 0, measure: 226\n",
      "duration error: 30.25, inst: 0, measure: 268\n",
      "duration error: 31.75, inst: 0, measure: 277\n",
      "duration error: 30.25, inst: 0, measure: 283\n",
      "duration error: 31.75, inst: 0, measure: 284\n",
      "duration error: 32.25, inst: 0, measure: 304\n",
      "duration error: 34.0, inst: 0, measure: 304\n",
      "duration error: 30.25, inst: 0, measure: 321\n",
      "duration error: 32.25, inst: 0, measure: 366\n",
      "duration error: 30.25, inst: 0, measure: 398\n",
      "duration error: 30.25, inst: 0, measure: 400\n",
      "duration error: 31.75, inst: 0, measure: 404\n",
      "duration error: 32.25, inst: 0, measure: 425\n",
      "duration error: 30.25, inst: 0, measure: 426\n",
      "duration error: 31.75, inst: 0, measure: 433\n",
      "duration error: 30.25, inst: 0, measure: 459\n",
      "duration error: 31.25, inst: 0, measure: 481\n",
      "duration error: 30.25, inst: 0, measure: 497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affd55b529c34af0bc1a306d7186e9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 31.75, inst: 1, measure: 6\n",
      "duration error: 31.75, inst: 1, measure: 7\n",
      "duration error: 31.75, inst: 1, measure: 36\n",
      "duration error: 31.75, inst: 1, measure: 133\n",
      "duration error: 31.75, inst: 1, measure: 156\n",
      "duration error: 31.75, inst: 1, measure: 156\n",
      "duration error: 30.25, inst: 1, measure: 214\n",
      "duration error: 31.25, inst: 1, measure: 250\n",
      "duration error: 31.75, inst: 1, measure: 250\n",
      "duration error: 35.25, inst: 1, measure: 264\n",
      "duration error: 31.75, inst: 1, measure: 271\n",
      "duration error: 31.75, inst: 1, measure: 339\n",
      "duration error: 30.25, inst: 1, measure: 379\n",
      "duration error: 30.25, inst: 1, measure: 394\n",
      "duration error: 30.25, inst: 1, measure: 412\n",
      "duration error: 32.25, inst: 1, measure: 416\n",
      "duration error: 31.75, inst: 1, measure: 421\n",
      "duration error: 31.25, inst: 1, measure: 437\n",
      "duration error: 31.75, inst: 1, measure: 451\n",
      "duration error: 32.25, inst: 1, measure: 469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1561880cbe1348bab08f9aa4fd990299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 31.75, inst: 2, measure: 9\n",
      "duration error: 32.25, inst: 2, measure: 28\n",
      "duration error: 30.25, inst: 2, measure: 81\n",
      "duration error: 32.5, inst: 2, measure: 90\n",
      "duration error: 31.75, inst: 2, measure: 94\n",
      "duration error: 31.75, inst: 2, measure: 177\n",
      "duration error: 31.75, inst: 2, measure: 209\n",
      "duration error: 31.75, inst: 2, measure: 280\n",
      "duration error: 31.75, inst: 2, measure: 301\n",
      "duration error: 30.25, inst: 2, measure: 338\n",
      "duration error: 31.75, inst: 2, measure: 373\n",
      "duration error: 32.25, inst: 2, measure: 428\n",
      "duration error: 31.75, inst: 2, measure: 492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e6a8310594ec18bfd2ad4813c52b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 32.25, inst: 3, measure: 124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1443b608999438ca0717268773326b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 31.5, inst: 4, measure: 317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbe1d18c53d4ddb995e33418587528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration error: 32.5, inst: 5, measure: 25\n",
      "duration error: 31.0, inst: 5, measure: 162\n",
      "duration error: 30.5, inst: 5, measure: 193\n",
      "duration error: 30.5, inst: 5, measure: 193\n",
      "duration error: 32.25, inst: 5, measure: 228\n",
      "duration error: 32.75, inst: 5, measure: 236\n"
     ]
    }
   ],
   "source": [
    "print(\"Start inference\")\n",
    "score = stream.Score(id='mainScore')\n",
    "\n",
    "prev_generation = None\n",
    "outputs = []\n",
    "source_part = stream.Part()\n",
    "merged_part = stream.Part() \n",
    "srcs = []\n",
    "\n",
    "\n",
    "for target_idx in range(6):\n",
    "  outputs = []\n",
    "  prev_generation = None\n",
    "  for i in tqdm(range(len(test_set))):\n",
    "    sample, _, _ = test_set[i]\n",
    "    src, output_decoded, (attention_map, output, new_out) = model.shifted_inference(sample, target_idx, prev_generation=prev_generation)\n",
    "    final_measure_num =  output[-1][-1]-3\n",
    "    while final_measure_num != 4:\n",
    "      src, output_decoded, (attention_map, output, new_out) = model.shifted_inference(sample, target_idx, prev_generation=prev_generation)\n",
    "      final_measure_num =  output[-1][-1]-3\n",
    "    if target_idx == 0 and i % test_set.slice_measure_number == 0:\n",
    "      srcs.append(src)\n",
    "    if i == 0:\n",
    "      sel_out = torch.cat([output[0:1]] + [get_measure_specific_output(output, i) for i in range(3,6)], dim=0)\n",
    "    else:\n",
    "      sel_out = get_measure_specific_output(output, 5) \n",
    "      duration = sum([note[2] for note in model.converter(sel_out)])\n",
    "      while abs(duration - 30) > 0.1:\n",
    "        print(f\"duration error: {duration}, inst: {target_idx}, measure: {i}\")\n",
    "        src, output_decoded, (attention_map, output, new_out) = model.shifted_inference(sample, target_idx, prev_generation=prev_generation)\n",
    "        final_measure_num =  output[-1][-1]-3\n",
    "        if final_measure_num != 4:\n",
    "          continue\n",
    "        sel_out = get_measure_specific_output(output, 5)\n",
    "        duration = sum([note[2] for note in model.converter(sel_out)])\n",
    "\n",
    "    prev_generation = get_measure_shifted_output(output)\n",
    "    outputs.append(sel_out)\n",
    "\n",
    "  if target_idx == 0:\n",
    "    srcs.append(fill_in_source(src, i % test_set.slice_measure_number))\n",
    "    source_converted = [y for x in srcs for y in x]\n",
    "    source_part = source_decoder(source_converted[1:])\n",
    "    score.insert(0, source_part)\n",
    "\n",
    "  outputs.append(get_measure_specific_output(output, 6)) # add last measure\n",
    "  outputs_tensor = torch.cat(outputs, dim=0)\n",
    "  final_midi = decoder(model.converter(outputs_tensor))\n",
    "  score.insert(0, final_midi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/teo/userdata/git_libraries/Yeominrak/chihwapyeong_orchestraion_final.musicxml')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.write('musicxml', fp='chihwapyeong_orchestraion_final.musicxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_measure_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f467c92250654003abd0a47cdd2c8492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ca85634de64c329486770b6b6e4ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9d723c6eab4cd582d5f7e4a1c557b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c51f59a677f4321b11a2b2cfe45c0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0262d74ddd6a46e7a21562de39298fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_set))):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# for i in tqdm(range(20)):\u001b[39;00m\n\u001b[1;32m     16\u001b[0m   sample, _, _ \u001b[38;5;241m=\u001b[39m test_set[i]\n\u001b[0;32m---> 17\u001b[0m   src, output_decoded, (attention_map, output, new_out) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshifted_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_generation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m target_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m test_set\u001b[38;5;241m.\u001b[39mslice_measure_number \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     srcs\u001b[38;5;241m.\u001b[39mappend(src)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/userdata/git_libraries/Yeominrak/model_zoo.py:836\u001b[0m, in \u001b[0;36mQkvAttnSeq2seqOrch.shifted_inference\u001b[0;34m(self, src, part_idx, prev_generation)\u001b[0m\n\u001b[1;32m    834\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39m_get_embedding(selected_token)\n\u001b[1;32m    835\u001b[0m decode_out, last_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mrnn(emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), last_hidden)\n\u001b[0;32m--> 836\u001b[0m attention_vectors, attention_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_attention_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m combined_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([decode_out, attention_vectors], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    838\u001b[0m selected_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msampling_process(combined_value)\n",
      "File \u001b[0;32m~/userdata/git_libraries/Yeominrak/model_zoo.py:743\u001b[0m, in \u001b[0;36mQkvAttnSeq2seq._get_attention_vector\u001b[0;34m(self, encoder_hidden_states, decoder_hidden_states, mask)\u001b[0m\n\u001b[1;32m    741\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mquery(decoder_hidden_states)\n\u001b[1;32m    742\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mkey(encoder_hidden_states)\n\u001b[0;32m--> 743\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m attention_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(key, query\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Start inference\")\n",
    "score = stream.Score(id='mainScore')\n",
    "\n",
    "prev_generation = None\n",
    "outputs = []\n",
    "source_part = stream.Part()\n",
    "merged_part = stream.Part() \n",
    "srcs = []\n",
    "\n",
    "for target_idx in range(6):\n",
    "# for target_idx in range(1):\n",
    "  outputs = []\n",
    "  prev_generation = None\n",
    "  for i in tqdm(range(len(test_set))):\n",
    "  # for i in tqdm(range(20)):\n",
    "    sample, _, _ = test_set[i]\n",
    "    src, output_decoded, (attention_map, output, new_out) = model.shifted_inference(sample, target_idx, prev_generation=prev_generation)\n",
    "    if target_idx == 0 and i % test_set.slice_measure_number == 0:\n",
    "      srcs.append(src)\n",
    "    if i == 0:\n",
    "      sel_out = torch.cat([output[0:1]] + [get_measure_specific_output(output, i) for i in range(3,6)], dim=0)\n",
    "    else:\n",
    "      sel_out = get_measure_specific_output(output, 5) \n",
    "      duration = sum([note[2] for note in model.converter(sel_out)])\n",
    "    prev_generation = get_measure_shifted_output(output)\n",
    "    outputs.append(sel_out)\n",
    "\n",
    "  if target_idx == 0:\n",
    "    srcs.append(fill_in_source(src, i % test_set.slice_measure_number))\n",
    "    source_converted = [y for x in srcs for y in x]\n",
    "    source_part = source_decoder(source_converted[1:])\n",
    "    score.insert(0, source_part)\n",
    "\n",
    "  outputs.append(get_measure_specific_output(output, 6)) # add last measure\n",
    "  outputs_tensor = torch.cat(outputs, dim=0)\n",
    "  final_midi = decoder(model.converter(outputs_tensor))\n",
    "  score.insert(0, final_midi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OrchestraScore(is_valid= False, \n",
    "                              xml_path='yeominlak_omr5.musicxml',\n",
    "                              use_pitch_modification = config.data.use_pitch_modification, \n",
    "                              pitch_modification_ratio=config.data.modification_ratio,\n",
    "                              min_meas=config.data.min_meas, \n",
    "                              max_meas=config.data.max_meas,\n",
    "                              feature_types=config.model.features,\n",
    "                              sampling_rate=config.data.sampling_rate,\n",
    "                              slice_measure_num=config.data.max_meas\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 26, 27,  3,  3,  7])\n",
      "tensor([ 0, 27,  6,  3,  3,  7])\n",
      "tensor([ 0, 27,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  7,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 19, 10,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 22,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 22,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 20, 10,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 19,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 18,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 18, 10,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 22,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20, 10,  3,  3,  7])\n",
      "tensor([ 0, 23,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 20,  6,  3,  3,  7])\n",
      "tensor([ 0, 18,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 26,  6,  3,  3,  7])\n",
      "tensor([ 0, 19, 10,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 22,  6,  3,  3,  7])\n",
      "tensor([ 0, 16, 15,  3,  3,  7])\n",
      "tensor([ 0, 22,  8,  3,  3,  7])\n",
      "tensor([ 0, 22, 10,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 19, 11,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 26, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  8,  3,  3,  7])\n",
      "tensor([ 0, 19, 11,  3,  3,  7])\n",
      "tensor([ 0, 18, 15,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 19,  8,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 19, 15,  3,  3,  7])\n",
      "tensor([ 0, 26,  8,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 28,  8,  3,  3,  7])\n",
      "tensor([ 0, 26, 23,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 27, 10,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 26,  8,  3,  3,  7])\n",
      "tensor([ 0, 26, 15,  3,  3,  7])\n",
      "tensor([ 0, 21,  6,  3,  3,  7])\n",
      "tensor([ 0, 22, 15,  3,  3,  7])\n",
      "tensor([ 0, 20,  8,  3,  3,  7])\n",
      "tensor([ 0, 20, 23,  3,  3,  7])\n",
      "tensor([ 0, 27, 10,  3,  3,  7])\n",
      "tensor([ 0, 20, 23,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 22,  8,  3,  3,  7])\n",
      "tensor([ 0, 19,  8,  3,  3,  7])\n",
      "tensor([ 0, 20, 23,  3,  3,  7])\n",
      "tensor([ 0, 19, 23,  3,  3,  7])\n",
      "tensor([ 0, 23,  8,  3,  3,  7])\n",
      "tensor([ 0, 28, 23,  3,  3,  7])\n",
      "tensor([ 0, 20, 23,  3,  3,  7])\n",
      "tensor([ 0, 18,  8,  3,  3,  7])\n",
      "tensor([ 0, 22,  8,  3,  3,  7])\n",
      "tensor([ 0, 26, 10,  3,  3,  7])\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(train_dataset)):\n",
    "  _, tgt, _ = train_dataset.get_processed_feature(7, 0, idx)\n",
    "  print(tgt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.slice_measure_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 'start', 'start', 0.0, 'strong', 0], [6, 46.0, 6.0, 6.0, 'weak', 0], [6, 39.0, 3.0, 9.0, 'middle', 0], [6, 46.0, 6.0, 15.0, 'strong', 0], [6, 46.0, 6.0, 21.0, 'middle', 0], [6, 46.0, 3.0, 24.0, 'weak', 0], [6, 44.0, 6.0, 0.0, 'strong', 1], [6, 56.0, 6.0, 6.0, 'weak', 1], [6, 53.0, 3.0, 9.0, 'middle', 1], [6, 51.0, 6.0, 15.0, 'strong', 1], [6, 46.0, 6.0, 21.0, 'middle', 1], [6, 48.0, 3.0, 24.0, 'weak', 1], [6, 53.0, 3.0, 27.0, 'weak', 1], [6, 56.0, 3.0, 0.0, 'strong', 2], [6, 51.0, 6.0, 6.0, 'weak', 2], [6, 39.0, 3.0, 9.0, 'middle', 2], [6, 51.0, 6.0, 15.0, 'strong', 2], [6, 51.0, 6.0, 21.0, 'middle', 2], [6, 51.0, 3.0, 24.0, 'weak', 2], [6, 53.0, 6.0, 0.0, 'strong', 3], [6, 51.0, 6.0, 6.0, 'weak', 3], [6, 48.0, 3.0, 9.0, 'middle', 3], [6, 51.0, 6.0, 15.0, 'strong', 3], [6, 56.0, 6.0, 21.0, 'middle', 3], [6, 53.0, 3.0, 24.0, 'weak', 3], [6, 51.0, 6.0, 0, 'strong', 4]]\n",
      "[[6, 'start', 'start', 0.0, 'strong', 0], [6, 60.0, 6.0, 6.0, 'weak', 0], [6, 65.0, 3.0, 9.0, 'middle', 0], [6, 46.0, 6.0, 15.0, 'strong', 0], [6, 46.0, 6.0, 21.0, 'middle', 0], [6, 82.0, 3.0, 24.0, 'weak', 0], [6, 44.0, 6.0, 0.0, 'strong', 1], [6, 56.0, 6.0, 6.0, 'weak', 1], [6, 53.0, 3.0, 9.0, 'middle', 1], [6, 51.0, 6.0, 15.0, 'strong', 1], [6, 46.0, 6.0, 21.0, 'middle', 1], [6, 48.0, 3.0, 24.0, 'weak', 1], [6, 53.0, 3.0, 27.0, 'weak', 1], [6, 65.0, 3.0, 0.0, 'strong', 2], [6, 51.0, 6.0, 6.0, 'weak', 2], [6, 39.0, 3.0, 9.0, 'middle', 2], [6, 51.0, 6.0, 15.0, 'strong', 2], [6, 51.0, 6.0, 21.0, 'middle', 2], [6, 51.0, 3.0, 24.0, 'weak', 2], [6, 53.0, 6.0, 0.0, 'strong', 3], [6, 51.0, 6.0, 6.0, 'weak', 3], [6, 48.0, 3.0, 9.0, 'middle', 3], [6, 51.0, 6.0, 15.0, 'strong', 3], [6, 56.0, 6.0, 21.0, 'middle', 3], [6, 85.0, 3.0, 24.0, 'weak', 3], [6, 51.0, 6.0, 0, 'strong', 4]]\n",
      "torch.Size([26, 6]) tensor([ 5, 10, 23,  3,  3,  7])\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(train_dataset)):\n",
    "  source, _, shifted_tgt = train_dataset.get_processed_feature(7, 5, 100)\n",
    "  print(shifted_tgt.shape, shifted_tgt[-2])\n",
    "  # print(source.shape, source[-2])\n",
    "  break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
