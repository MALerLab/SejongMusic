{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/teo/userdata\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from fractions import Fraction\n",
    "\n",
    "import x_transformers\n",
    "from omegaconf import OmegaConf\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence, pad_packed_sequence, pack_padded_sequence, pad_sequence \n",
    "\n",
    "from sejong_music.model_zoo import get_emb_total_size\n",
    "from sejong_music.yeominrak_processing import ShiftedAlignedScore, Tokenizer\n",
    "from sejong_music.loss import nll_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumEmbedding(nn.Module):\n",
    "  def __init__(self, vocab_sizes: dict, emb_size: int) -> None:\n",
    "    super().__init__()\n",
    "    self.layers = []\n",
    "    self.emb_size = emb_size\n",
    "    for vocab_size in vocab_sizes.values():\n",
    "      self.layers.append(nn.Embedding(vocab_size, self.emb_size))\n",
    "    self.layers = nn.ModuleList(self.layers)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return torch.sum(torch.stack([module(x[..., i]) for i, module in enumerate(self.layers)], dim=-1), dim=-1)\n",
    "\n",
    "multiemb = SumEmbedding({'a': 10, 'b': 20}, 4)\n",
    "dummy = torch.randint(0, 9, (17, 31, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_transformer(raw_batch):\n",
    "  # source, target= zip(*raw_batch)[0], zip(*raw_batch)[1]\n",
    "    source, target, shi_target = zip(*raw_batch)\n",
    "    padded_src = pad_sequence(source, batch_first=True, padding_value=0)\n",
    "    padded_target = pad_sequence(target, batch_first=True, padding_value=0)\n",
    "    shi_target = pad_sequence(shi_target, batch_first=True, padding_value=0)\n",
    "    return [padded_src, padded_target, shi_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShiftedAlignedScore(xml_path='music_score/yeominlak.musicxml')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 39, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, tgt, shi_tgt = next(iter(dataloader))\n",
    "vocab_size_dict = dataset.tokenizer.vocab_size_dict\n",
    "vocab_size_dict\n",
    "config = OmegaConf.load('yamls/transformer.yaml')\n",
    "# config= get_emb_total_size(config)\n",
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransSeq2seq\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, config):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TransSeq2seq(nn.Module):\n",
    "  def __init__(self, tokenizer, config):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab_size_dict = self.tokenizer.vocab_size_dict\n",
    "    self.encoder = Encoder(self.vocab_size_dict, config)\n",
    "    self.decoder = Decoder(self.vocab_size_dict, config)\n",
    "  \n",
    "  def forward(self, src, tgt):\n",
    "    enc_out, src_mask = self.encoder(src)\n",
    "    dec_out = self.decoder(tgt, enc_out, src_mask)\n",
    "    return dec_out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size_dict: dict, config):\n",
    "    super().__init__()\n",
    "    self.param = config\n",
    "    self.vocab_size = [x for x in vocab_size_dict.values()]\n",
    "    self.vocab_size_dict = vocab_size_dict\n",
    "    self._make_embedding_layer()\n",
    "    self.layers = x_transformers.Encoder(dim=self.param.dim ,depth=self.param.depth, num_heads=self.param.num_heads)\n",
    "    \n",
    "  def _make_embedding_layer(self):\n",
    "    self.embedding = SumEmbedding(self.vocab_size_dict, self.param.dim)\n",
    "    \n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    x: (batch, seq_len, num_features)\n",
    "    '''\n",
    "    mask = (x != 0)[..., 0] # squeeze num_features dimension\n",
    "    embedding = self.embedding(x)\n",
    "    return self.layers(embedding, mask=mask), mask\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size_dict: dict, config):\n",
    "    super().__init__()\n",
    "    self.vocab_size = [x for x in vocab_size_dict.values()]\n",
    "    self.param = config\n",
    "    self.embedding = SumEmbedding(vocab_size_dict, config.dim)\n",
    "    self.layers = x_transformers.Decoder(dim=config.dim, depth=config.depth, num_heads=config.num_heads, cross_attend=True)\n",
    "    self._make_projection_layer()\n",
    "    \n",
    "  def forward(self, x, enc_out, src_mask):\n",
    "    mask = (x != 0)[..., 0]\n",
    "    embedding = self.embedding(x)\n",
    "    output = self.layers(embedding, context=enc_out, mask=mask, context_mask=src_mask)\n",
    "    logit = self.proj(output)\n",
    "    dec_out = self._apply_softmax(logit)\n",
    "    return dec_out\n",
    "  \n",
    "  def _make_projection_layer(self):\n",
    "    # total_vocab_size = sum([x for x in self.vocab_size_dict.values()])\n",
    "    self.proj = nn.Linear(self.param.dim, self.vocab_size[1] + self.vocab_size[2])\n",
    "\n",
    "  def _apply_softmax(self, logit):\n",
    "    pitch_output = torch.softmax(logit[...,:self.vocab_size[1]], dim=-1)\n",
    "    # print(logit.shape)\n",
    "    dur_output = torch.softmax(logit[...,self.vocab_size[1] :], dim=-1)\n",
    "    # print(f\"{pitch_output.shape, dur_output.shape}!!\")\n",
    "    return torch.cat([pitch_output, dur_output], dim=-1)\n",
    "  \n",
    "  def _select_token(self, prob: torch.Tensor):\n",
    "    tokens = []\n",
    "    # print(prob.shape)\n",
    "    pitch_token = prob[0, :, :self.vocab_size[1]].multinomial(num_samples=1)\n",
    "    dur_token = prob[0, :, self.vocab_size[1]:].multinomial(num_samples=1)\n",
    "    # dur_token = torch.argmax(prob.data[:, model.vocab_size[0]:], dim=-1)\n",
    "    return torch.cat([pitch_token, dur_token], dim=-1)\n",
    "  \n",
    "model = TransSeq2seq(dataset.tokenizer, config.model)\n",
    "prob = model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  5, 12,  ...,  0,  0,  0],\n",
       "        [ 9, 12,  5,  ...,  0,  0,  0],\n",
       "        [ 7, 10,  9,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 5,  6,  6,  ...,  0,  0,  0],\n",
       "        [ 5,  6,  6,  ...,  0,  0,  0],\n",
       "        [ 8,  6,  8,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shi_tgt[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob == prob.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(pred, target, eps=1e-8):\n",
    "  if pred.ndim == 3:\n",
    "    pred = pred.flatten(0, 1)\n",
    "  if target.ndim > 1:\n",
    "    target = target.flatten()\n",
    "  mask = target != 0\n",
    "  return (-torch.log(pred[torch.arange(len(target)), target] + eps) * mask) / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.7003,  0.1071,  0.9161,  0.3386, -0.2893,  1.2277,  0.4057,\n",
       "           -0.7975,  0.9658, -1.0138,  0.8950,  1.5130, -0.2053,  0.2745,\n",
       "           -0.4462, -0.4711,  0.1771,  1.4812, -1.4967,  0.9481,  0.3569,\n",
       "            0.1669, -0.8546, -0.7160,  0.3229,  0.6983, -0.2871, -1.6515,\n",
       "           -0.0788, -1.4268, -1.9439,  0.0548, -0.7643, -1.0192, -2.1964,\n",
       "           -0.6969, -1.2136,  0.6118,  0.6727, -0.9330,  1.1635, -0.0994,\n",
       "           -0.8573, -0.0378,  0.4520,  0.8292, -0.2709,  0.8911, -0.6651,\n",
       "            0.7408, -1.2015, -0.1403, -0.1171, -0.7316, -1.9446,  0.7199,\n",
       "            0.2564,  0.4507, -0.3684,  0.7075,  1.9839,  0.1413, -0.3816,\n",
       "            3.1468],\n",
       "          [-0.1188, -1.1370,  1.0609, -0.1823, -0.4064,  2.0915, -1.0626,\n",
       "            0.8713,  0.9061, -0.6444, -0.9870,  1.0683,  0.3534, -1.4500,\n",
       "            0.0784,  1.8547, -0.7242,  1.0078,  0.2001, -1.4425, -0.2128,\n",
       "           -0.2776, -1.6377,  0.2994, -1.2781, -1.4076, -0.1014,  0.0884,\n",
       "            0.1804,  0.7745,  0.0583,  1.4914,  0.9122, -0.1638,  1.4721,\n",
       "           -0.4499,  0.5712,  0.8463, -0.9801,  0.0041,  0.8782,  0.2112,\n",
       "           -1.7143,  0.9154,  1.0701,  1.7962, -0.1270,  1.4533, -1.7384,\n",
       "            0.2400, -0.2306, -1.1160, -0.2291, -1.4142,  1.1827,  1.4531,\n",
       "           -0.3153, -2.1637,  0.3381, -0.6705,  0.2516,  0.0289, -0.9489,\n",
       "           -0.6775],\n",
       "          [ 0.5425, -1.1890,  0.3411, -1.3635, -0.7813, -0.3265,  0.4987,\n",
       "           -0.3933, -0.5954,  0.9672,  2.1169,  0.8805,  0.1634,  0.1197,\n",
       "            0.9731, -0.0979, -0.4675,  0.8720, -0.2448,  1.3515,  0.3749,\n",
       "            1.0933,  1.3075,  0.1965,  0.4935,  0.2899,  2.7613, -0.3870,\n",
       "           -0.3584, -0.7496,  0.5760,  0.1894, -0.0439,  0.1113, -2.2166,\n",
       "           -1.4341, -0.6953,  1.0089,  0.1900, -1.0480,  0.5975, -0.9446,\n",
       "           -0.4907,  1.4177, -0.6945,  1.1178, -0.8856,  0.3254, -0.6155,\n",
       "            0.9240, -0.2961, -1.8342, -1.6024, -0.2169, -0.6128,  0.1272,\n",
       "            2.4090, -1.5115,  0.5493, -1.2156,  0.8225, -0.1531, -1.2353,\n",
       "           -1.0081]],\n",
       " \n",
       "         [[ 0.0193, -0.1683,  1.7747, -0.5464,  0.1623,  0.8306, -0.1715,\n",
       "            0.1326,  0.4410,  0.4856, -1.2687,  0.8574,  0.7080, -0.2835,\n",
       "            0.9685,  0.4443, -0.8852,  0.7025, -0.1705, -0.2815, -0.3725,\n",
       "           -0.3248,  0.5612,  0.1653,  0.4441, -0.7955,  1.2360, -0.0057,\n",
       "            1.4482, -0.3716,  0.7493,  0.7390,  0.7295,  0.0728, -0.8443,\n",
       "           -1.6014, -0.7260, -0.6456, -0.2445,  0.0649,  2.6835,  1.0834,\n",
       "           -0.6871,  1.2392, -0.9390,  1.5716, -1.0625, -0.2744, -1.9246,\n",
       "           -2.0751, -1.3748, -0.6903,  0.1221, -0.7769, -0.1318,  2.7211,\n",
       "           -0.6813, -1.8434,  0.5445, -0.6671,  0.4478,  0.7159, -2.0248,\n",
       "           -0.0055],\n",
       "          [-0.6031, -0.0384,  0.4486, -0.2475, -0.4565,  2.3560,  0.9284,\n",
       "           -0.6241, -0.8946, -0.3689, -0.0220,  0.5571,  0.8648,  1.6055,\n",
       "           -2.5002, -0.2560,  0.0695,  0.6032,  1.0429, -1.9258, -0.9396,\n",
       "            0.4317, -0.4597, -0.2116, -0.0957, -0.9345, -0.4581,  1.8326,\n",
       "           -0.8928,  0.2601, -0.3172,  2.2688,  0.4530,  0.6876, -0.3563,\n",
       "           -0.4658,  0.2949, -0.6522,  1.6305,  1.4970, -0.1267, -0.0897,\n",
       "           -0.4341,  0.4236, -0.1956,  0.4557, -1.2258,  0.9110, -0.4560,\n",
       "           -2.9029, -0.3455, -1.1955,  0.8040, -1.1542,  0.5195,  0.5906,\n",
       "            0.1242, -0.7492,  1.3239,  1.5655,  0.0848, -0.7530, -0.4826,\n",
       "           -0.8036],\n",
       "          [-0.4288,  0.1646, -0.4931, -1.3466,  0.6570,  0.9734,  1.0806,\n",
       "           -0.3950, -0.1768,  0.3297,  1.2495,  1.0611,  0.0046, -0.3373,\n",
       "            1.0921,  1.3573, -1.1302, -0.6354, -1.7158,  1.8741,  1.4797,\n",
       "            0.5183,  0.0145, -0.4811, -1.6529, -1.4381,  1.1538,  2.9765,\n",
       "           -1.2442,  0.2708, -2.1143,  0.4371, -0.3871,  0.6804, -0.7940,\n",
       "           -0.4022,  0.1085,  0.7236,  0.4470, -0.2056, -0.6866,  0.9923,\n",
       "           -0.9134,  0.0081, -1.0227, -0.3151,  0.2421,  0.0502, -0.8519,\n",
       "           -0.5591, -0.3833, -0.8626, -0.2665, -0.7434,  1.0321, -0.2388,\n",
       "           -0.5458, -0.8557,  1.2090,  2.7421,  0.4004, -1.1019, -0.1809,\n",
       "           -0.4247]],\n",
       " \n",
       "         [[-0.2174, -0.5311,  0.9328, -0.8832,  1.3464, -0.5102, -1.0616,\n",
       "           -0.4079, -0.0313,  1.1121,  1.0312, -0.2829,  0.7263,  2.1681,\n",
       "            0.4821, -0.2716,  0.6149,  0.9708, -0.0956,  0.7661,  2.3558,\n",
       "           -0.3436,  0.0072, -0.1915,  0.1615, -1.5425,  0.4967, -0.6350,\n",
       "            0.5973,  0.9410, -1.3346, -0.4059, -0.4026,  0.3277,  0.8787,\n",
       "            0.1622, -0.3209, -0.0647, -0.5447, -1.4562,  0.3126,  0.3445,\n",
       "           -1.6088,  0.2090,  1.7438,  1.4601, -1.0499,  1.2278,  0.8197,\n",
       "            0.0142,  0.2743, -1.7396, -1.4244, -1.1070, -0.0337, -0.4225,\n",
       "           -0.1019, -1.0546, -0.7375, -0.3135,  2.1837,  0.2587, -0.9970,\n",
       "           -2.8016],\n",
       "          [ 0.4105, -0.0741,  0.1287,  1.0887,  0.4427,  2.1285, -0.7414,\n",
       "            0.0979,  0.7106,  0.0704,  1.3599, -0.3974,  0.4201, -2.1207,\n",
       "           -0.1184,  2.7183, -0.3938,  1.3948, -0.4830,  0.6899, -0.1675,\n",
       "            0.5444, -0.8869,  1.4724, -1.7585, -0.7157,  1.0695, -0.3647,\n",
       "            0.4658, -1.6271, -0.2988, -1.0933,  0.8830, -1.8168, -0.0480,\n",
       "           -0.9536, -0.0559,  0.6527,  0.5563, -0.4870,  0.6040, -0.1301,\n",
       "            0.2715, -0.0327,  0.0076,  1.9505,  0.1527, -0.8607, -0.0751,\n",
       "            0.6513,  0.5687, -0.6910, -0.6108, -0.8468, -1.0176,  1.3547,\n",
       "            0.4551, -1.5218, -0.0470, -2.0500,  0.4581, -1.5786, -0.8500,\n",
       "            1.1357],\n",
       "          [ 0.7361,  1.2159,  0.8980,  0.4502,  1.8891,  1.8192, -0.8228,\n",
       "            0.8879, -0.6887,  1.2581, -0.0408,  0.7023,  0.7981,  0.5977,\n",
       "           -0.1156, -0.9340,  1.3738,  1.1353, -2.3186, -0.7199, -0.0242,\n",
       "            1.4659, -0.1676, -0.0059, -0.6165, -2.5267,  0.7024,  0.0619,\n",
       "           -1.1925,  1.1563, -0.3355, -1.4489, -1.5539, -1.4501,  1.1375,\n",
       "            0.7836, -0.0143, -1.8406, -0.5627, -1.1410,  1.2013, -0.0862,\n",
       "            0.4331, -0.0150, -0.0373,  1.4381, -0.7681, -0.6335,  0.6070,\n",
       "           -0.8983,  0.4677,  0.1852, -0.6048, -1.6159, -0.0369, -0.1529,\n",
       "           -0.4539, -0.7870, -0.1806, -0.5184,  0.0801,  1.6974, -0.1906,\n",
       "            0.3209]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[False,  True, False],\n",
       "         [False,  True,  True],\n",
       "         [ True,  True,  True]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.randint(0, 6, (3,3,6))\n",
    "model.encoder(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nll_loss\n",
    "pitch_loss = loss_fn(prob[..., :vocab_size[1]], shi_tgt[..., 1])\n",
    "dur_loss = loss_fn(prob[..., vocab_size[1]:], shi_tgt[..., 2])\n",
    "total_loss = (pitch_loss + dur_loss) / 2         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_inference(self, src, part_idx, prev_generation=None, fix_first_beat=False, compensate_beat=(0.0, 0.0)):\n",
    "    dev = src.device\n",
    "    assert src.ndim == 2\n",
    "    enc_out, enc_mask = self.encoder(src.unsqueeze(0)) \n",
    "\n",
    "    # Setup for 0th step\n",
    "    # start_token = torch.LongTensor([[part_idx, 1, 1, 3, 3, 4]]) # start token idx is 1\n",
    "    start_token = self.get_start_token(part_idx).to(dev)\n",
    "  \n",
    "    #-------------\n",
    "    # not_triplet_vocab_list = [ i for i, dur in enumerate(self.tokenizer.vocab['duration'][3:], 3) if (dur%0.5) != 0.0]\n",
    "    #-------------\n",
    "    measure_duration = self._get_measure_duration(part_idx)\n",
    "    current_measure_idx = 0\n",
    "    measure_changed = 1\n",
    "\n",
    "    final_tokens = [start_token]\n",
    "    if prev_generation is not None:\n",
    "      final_tokens, current_measure_idx, last_hidden = self._apply_prev_generation(prev_generation, final_tokens, last_hidden, enc_out)\n",
    "      # emb = self.decoder._get_embedding(prev_generation)\n",
    "      # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "      # start_token = torch.cat([prev_generation[-1:, :3].to(dev),  torch.LongTensor([[3, 3, 5]]).to(dev)], dim=-1)\n",
    "      # final_tokens.append(start_token)\n",
    "      # current_measure_idx = 2\n",
    "    selected_token = start_token\n",
    "    \n",
    "    current_beat = Fraction(0, 1)\n",
    "\n",
    "    total_attention_weights = []\n",
    "    # while True:\n",
    "    triplet_sum = 0 \n",
    "    condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "\n",
    "\n",
    "    for i in range(300):\n",
    "      # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "      # attention_vectors, attention_weight = self._get_attention_vector(encode_out, decode_out)\n",
    "      # combined_value = torch.cat([decode_out, attention_vectors], dim=-1)\n",
    "      # combined_value, last_hidden, attention_weight = self._run_inference_on_step(emb, last_hidden, encode_out)\n",
    "      prob = self.decoder(torch.cat(final_tokens, dim=0).unsqueeze(0), enc_out, enc_mask)\n",
    "      selected_token = self.decoder._select_token(prob)\n",
    "      \n",
    "      # for rule_base fixing\n",
    "      if fix_first_beat and current_beat - compensate_beat[1] == 0.0:\n",
    "        for src_token in src:\n",
    "          if src_token[-1] == condition_tokens[-1] and self.tokenizer.vocab['offset'][src_token[3].item()] - compensate_beat[0] == 0.0:\n",
    "            # if measure_idx is same and offset is same\n",
    "            selected_token[0, 0] = src_token[1] # get pitch from src\n",
    "            break\n",
    "      \n",
    "      if selected_token[0, 0]==self.tokenizer.tok2idx['pitch']['end'] or selected_token[0, 1]==self.tokenizer.tok2idx['duration']['end']:\n",
    "        break\n",
    "      \n",
    "      # update beat position and beat strength\n",
    "      current_dur = self.tokenizer.vocab['duration'][selected_token[0, 1]]\n",
    "      if Fraction(current_dur).limit_denominator(3).denominator == 3:\n",
    "        triplet_sum += Fraction(current_dur).limit_denominator(3).numerator\n",
    "      elif triplet_sum != 0:\n",
    "        # print(\"Triplet has to appear but not\", current_dur,)\n",
    "        triplet_sum += 1\n",
    "        # print(current_dur, selected_token)\n",
    "        current_dur = Fraction(1,3)\n",
    "        selected_token = torch.LongTensor([[selected_token[0,0], self.tokenizer.tok2idx['duration'][current_dur]]]).to(dev)\n",
    "        # print(current_dur, selected_token)\n",
    "      else:\n",
    "        triplet_sum = 0\n",
    "      if triplet_sum == 3:\n",
    "        triplet_sum = 0\n",
    "      current_beat, current_measure_idx, measure_changed = self.update_condition_info(current_beat, current_measure_idx, measure_duration, current_dur)\n",
    "      if 'measure_idx' in self.tokenizer.tok2idx and current_measure_idx > self.tokenizer.vocab['measure_idx'][-1]:\n",
    "        break\n",
    "      if float(current_beat) == 10.0:\n",
    "        print(f\"Current Beat ==10.0 detected! cur_beat: {current_beat}, measure_dur: {measure_duration}, cur_measure_idx: {current_measure_idx}, cur_dur: {current_dur}, triplet_sum: {triplet_sum}\")\n",
    "      condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "      \n",
    "      # make new token for next rnn timestep\n",
    "      selected_token = torch.cat([torch.LongTensor([[part_idx]]).to(dev), selected_token, torch.LongTensor([condition_tokens]).to(dev)], dim=1)\n",
    "      \n",
    "      final_tokens.append(selected_token)\n",
    "      \n",
    "    if len(total_attention_weights) == 0:\n",
    "      attention_map = None\n",
    "    else:\n",
    "      attention_map = torch.stack(total_attention_weights, dim=1)\n",
    "      \n",
    "    cat_out = torch.cat(final_tokens[1:], dim=0) #token shape: [1,6] => [24,6]\n",
    "    newly_generated = cat_out.clone()\n",
    "    if prev_generation is not None:\n",
    "      cat_out = torch.cat([prev_generation[1:], cat_out], dim=0)\n",
    "    #  self.converter(src[1:-1]), self.converter(torch.cat(final_tokens, dim=0)), attention_map\n",
    "\n",
    "    return self._decode_inference_result(src, cat_out, (attention_map, cat_out, newly_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))\n\u001b[1;32m      2\u001b[0m out_a \u001b[38;5;241m=\u001b[39m encoder(batch[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m modified_x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "out_a = encoder(batch[0])\n",
    "\n",
    "modified_x = batch[0].clone()\n",
    "modified_x[0, 4, 1:] = 3\n",
    "out_b = encoder(modified_x)\n",
    "\n",
    "out_a == out_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (240) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mx: (batch, seq_len, num_features)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# squeeze num_features dimension\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/x_transformers.py:1334\u001b[0m, in \u001b[0;36mAttentionLayers.forward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, self_attn_kv_mask, mems, mem_masks, seq_start_pos, cache, cache_age, return_hiddens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         layer_mem \u001b[38;5;241m=\u001b[39m pre_norm(layer_mem)\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1334\u001b[0m     out, inter \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mself_attn_kv_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_attn_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer_mem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer_mem_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_intermediates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m layer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1336\u001b[0m     out, inter \u001b[38;5;241m=\u001b[39m block(x, context \u001b[38;5;241m=\u001b[39m context, mask \u001b[38;5;241m=\u001b[39m mask, context_mask \u001b[38;5;241m=\u001b[39m context_mask, prev_attn \u001b[38;5;241m=\u001b[39m prev_cross_attn, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_attn_cache, \u001b[38;5;28;01mNone\u001b[39;00m), return_intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/x_transformers.py:944\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, rel_pos, rotary_pos_emb, prev_attn, mem, mem_mask, return_intermediates, cache)\u001b[0m\n\u001b[1;32m    940\u001b[0m     attn_bias \u001b[38;5;241m=\u001b[39m rel_pos(i, j)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# attention is all we need\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m out, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinal_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# https://arxiv.org/abs/2208.06061 proposes to add a residual for better gradients\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(r):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/attend.py:300\u001b[0m, in \u001b[0;36mAttend.forward\u001b[0;34m(self, q, k, v, mask, attn_bias, prev_attn)\u001b[0m\n\u001b[1;32m    297\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m&\u001b[39m sparse_topk_mask) \u001b[38;5;28;01mif\u001b[39;00m exists(mask) \u001b[38;5;28;01melse\u001b[39;00m sparse_topk_mask\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(mask):\n\u001b[0;32m--> 300\u001b[0m     dots \u001b[38;5;241m=\u001b[39m \u001b[43mdots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m causal:\n\u001b[1;32m    303\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_causal_mask(i, j, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (240) at non-singleton dimension 3"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
