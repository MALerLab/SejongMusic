{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/danbi/userdata/DANBI/gugakwon/SejongMusic\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from fractions import Fraction\n",
    "\n",
    "import x_transformers\n",
    "from omegaconf import OmegaConf\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence, pad_packed_sequence, pack_padded_sequence, pad_sequence \n",
    "\n",
    "from sejong_music.model_zoo import get_emb_total_size, TransSeq2seq\n",
    "from sejong_music.yeominrak_processing import ShiftedAlignedScore, Tokenizer\n",
    "from sejong_music.loss import nll_loss\n",
    "from sejong_music.utils import pad_collate_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumEmbedding(nn.Module):\n",
    "  def __init__(self, vocab_sizes: dict, emb_size: int) -> None:\n",
    "    super().__init__()\n",
    "    self.layers = []\n",
    "    self.emb_size = emb_size\n",
    "    for vocab_size in vocab_sizes.values():\n",
    "      self.layers.append(nn.Embedding(vocab_size, self.emb_size))\n",
    "    self.layers = nn.ModuleList(self.layers)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return torch.sum(torch.stack([module(x[..., i]) for i, module in enumerate(self.layers)], dim=-1), dim=-1)\n",
    "\n",
    "multiemb = SumEmbedding({'a': 10, 'b': 20}, 4)\n",
    "dummy = torch.randint(0, 9, (17, 31, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_transformer(raw_batch):\n",
    "  # source, target= zip(*raw_batch)[0], zip(*raw_batch)[1]\n",
    "    source, target, shi_target = zip(*raw_batch)\n",
    "    padded_src = pad_sequence(source, batch_first=True, padding_value=0)\n",
    "    padded_target = pad_sequence(target, batch_first=True, padding_value=0)\n",
    "    shi_target = pad_sequence(shi_target, batch_first=True, padding_value=0)\n",
    "    return [padded_src, padded_target, shi_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShiftedAlignedScore(xml_path='music_score/yeominlak.musicxml')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=pad_collate_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 51, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, tgt, shi_tgt = next(iter(dataloader))\n",
    "vocab_size_dict = dataset.tokenizer.vocab_size_dict\n",
    "vocab_size_dict\n",
    "config = OmegaConf.load('yamls/transformer.yaml')\n",
    "# config= get_emb_total_size(config)\n",
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransSeq2seq(tokenizer=dataset.tokenizer, config=config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('outputs/2024-03-16/00-49-19/wandb/latest-run/files/checkpoints/epoch200_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransSeq2seq\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, config):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TransSeq2seq(nn.Module):\n",
    "  def __init__(self, tokenizer, config):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab_size_dict = self.tokenizer.vocab_size_dict\n",
    "    self.encoder = Encoder(self.vocab_size_dict, config)\n",
    "    self.decoder = Decoder(self.vocab_size_dict, config)\n",
    "  \n",
    "  def forward(self, src, tgt):\n",
    "    enc_out, src_mask = self.encoder(src)\n",
    "    dec_out = self.decoder(tgt, enc_out, src_mask)\n",
    "    return dec_out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size_dict: dict, config):\n",
    "    super().__init__()\n",
    "    self.param = config\n",
    "    self.vocab_size = [x for x in vocab_size_dict.values()]\n",
    "    self.vocab_size_dict = vocab_size_dict\n",
    "    self._make_embedding_layer()\n",
    "    self.layers = x_transformers.Encoder(dim=self.param.dim ,depth=self.param.depth, num_heads=self.param.num_heads)\n",
    "    \n",
    "  def _make_embedding_layer(self):\n",
    "    self.embedding = SumEmbedding(self.vocab_size_dict, self.param.dim)\n",
    "    \n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    x: (batch, seq_len, num_features)\n",
    "    '''\n",
    "    mask = (x != 0)[..., 0] # squeeze num_features dimension\n",
    "    embedding = self.embedding(x)\n",
    "    return self.layers(embedding, mask=mask), mask\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size_dict: dict, config):\n",
    "    super().__init__()\n",
    "    self.vocab_size = [x for x in vocab_size_dict.values()]\n",
    "    self.param = config\n",
    "    self.embedding = SumEmbedding(vocab_size_dict, config.dim)\n",
    "    self.layers = x_transformers.Decoder(dim=config.dim, depth=config.depth, num_heads=config.num_heads, cross_attend=True)\n",
    "    self._make_projection_layer()\n",
    "    \n",
    "  def forward(self, x, enc_out, src_mask):\n",
    "    mask = (x != 0)[..., 0]\n",
    "    embedding = self.embedding(x)\n",
    "    output = self.layers(embedding, context=enc_out, mask=mask, context_mask=src_mask)\n",
    "    logit = self.proj(output)\n",
    "    dec_out = self._apply_softmax(logit)\n",
    "    return dec_out\n",
    "  \n",
    "  def _make_projection_layer(self):\n",
    "    # total_vocab_size = sum([x for x in self.vocab_size_dict.values()])\n",
    "    self.proj = nn.Linear(self.param.dim, self.vocab_size[1] + self.vocab_size[2])\n",
    "\n",
    "  def _apply_softmax(self, logit):\n",
    "    pitch_output = torch.softmax(logit[...,:self.vocab_size[1]], dim=-1)\n",
    "    # print(logit.shape)\n",
    "    dur_output = torch.softmax(logit[...,self.vocab_size[1] :], dim=-1)\n",
    "    # print(f\"{pitch_output.shape, dur_output.shape}!!\")\n",
    "    return torch.cat([pitch_output, dur_output], dim=-1)\n",
    "  \n",
    "  def _select_token(self, prob: torch.Tensor):\n",
    "    tokens = []\n",
    "    # print(prob.shape)\n",
    "    pitch_token = prob[0, :, :self.vocab_size[1]].multinomial(num_samples=1)\n",
    "    dur_token = prob[0, :, self.vocab_size[1]:].multinomial(num_samples=1)\n",
    "    # dur_token = torch.argmax(prob.data[:, model.vocab_size[0]:], dim=-1)\n",
    "    return torch.cat([pitch_token, dur_token], dim=-1)\n",
    "  \n",
    "model = TransSeq2seq(dataset.tokenizer, config.model)\n",
    "prob = model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m source, _, _ \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshifted_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/userdata/DANBI/gugakwon/SejongMusic/sejong_music/model_zoo.py:1335\u001b[0m, in \u001b[0;36mTransSeq2seq.shifted_inference\u001b[0;34m(self, src, part_idx, prev_generation, fix_first_beat, compensate_beat)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   condition_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n\u001b[1;32m   1334\u001b[0m   \u001b[38;5;66;03m# make new token for next rnn timestep\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m   selected_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpart_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcondition_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m   final_tokens\u001b[38;5;241m.\u001b[39mappend(selected_token)\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(total_attention_weights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "source, _, _ = dataset[0]\n",
    "model.shifted_inference(source, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shifted_inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mshifted_inference \u001b[38;5;241m=\u001b[39m \u001b[43mshifted_inference\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shifted_inference' is not defined"
     ]
    }
   ],
   "source": [
    "model.shifted_inference = shifted_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "part_idx = 1\n",
    "prev_generation, compensate_beat, fix_first_beat = None, None, None\n",
    "src = source\n",
    "dev = src.device\n",
    "assert src.ndim == 2\n",
    "enc_out, enc_mask = self.encoder(src.unsqueeze(0)) \n",
    "\n",
    "# Setup for 0th step\n",
    "# start_token = torch.LongTensor([[part_idx, 1, 1, 3, 3, 4]]) # start token idx is 1\n",
    "start_token = self.get_start_token(part_idx).to(dev)\n",
    "\n",
    "#-------------\n",
    "# not_triplet_vocab_list = [ i for i, dur in enumerate(self.tokenizer.vocab['duration'][3:], 3) if (dur%0.5) != 0.0]\n",
    "#-------------\n",
    "measure_duration = self._get_measure_duration(part_idx)\n",
    "current_measure_idx = 0\n",
    "measure_changed = 1\n",
    "\n",
    "final_tokens = [start_token]\n",
    "if prev_generation is not None:\n",
    "  final_tokens, current_measure_idx, last_hidden = self._apply_prev_generation(prev_generation, final_tokens, last_hidden, enc_out)\n",
    "  # emb = self.decoder._get_embedding(prev_generation)\n",
    "  # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "  # start_token = torch.cat([prev_generation[-1:, :3].to(dev),  torch.LongTensor([[3, 3, 5]]).to(dev)], dim=-1)\n",
    "  # final_tokens.append(start_token)\n",
    "  # current_measure_idx = 2\n",
    "selected_token = start_token\n",
    "\n",
    "current_beat = Fraction(0, 1)\n",
    "\n",
    "total_attention_weights = []\n",
    "# while True:\n",
    "triplet_sum = 0 \n",
    "condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "\n",
    "for i in range(300):\n",
    "  # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "  # attention_vectors, attention_weight = self._get_attention_vector(encode_out, decode_out)\n",
    "  # combined_value = torch.cat([decode_out, attention_vectors], dim=-1)\n",
    "  # combined_value, last_hidden, attention_weight = self._run_inference_on_step(emb, last_hidden, encode_out)\n",
    "  prob = self.decoder(torch.cat(final_tokens, dim=0).unsqueeze(0), enc_out, enc_mask)\n",
    "  selected_token = self.decoder._select_token(prob[:, -1:])\n",
    "  # for rule_base fixing\n",
    "  if fix_first_beat and current_beat - compensate_beat[1] == 0.0:\n",
    "    for src_token in src:\n",
    "      if src_token[-1] == condition_tokens[-1] and self.tokenizer.vocab['offset'][src_token[3].item()] - compensate_beat[0] == 0.0:\n",
    "        # if measure_idx is same and offset is same\n",
    "        selected_token[0, 0] = src_token[1] # get pitch from src\n",
    "        break\n",
    "  \n",
    "  if selected_token[0, 0]==self.tokenizer.tok2idx['pitch']['end'] or selected_token[0, 1]==self.tokenizer.tok2idx['duration']['end']:\n",
    "    break\n",
    "  \n",
    "  # update beat position and beat strength\n",
    "  current_dur = self.tokenizer.vocab['duration'][selected_token[0, 1]]\n",
    "  if Fraction(current_dur).limit_denominator(3).denominator == 3:\n",
    "    triplet_sum += Fraction(current_dur).limit_denominator(3).numerator\n",
    "  elif triplet_sum != 0:\n",
    "    # print(\"Triplet has to appear but not\", current_dur,)\n",
    "    triplet_sum += 1\n",
    "    # print(current_dur, selected_token)\n",
    "    current_dur = Fraction(1,3)\n",
    "    selected_token = torch.LongTensor([[selected_token[0,0], self.tokenizer.tok2idx['duration'][current_dur]]]).to(dev)\n",
    "    # print(current_dur, selected_token)\n",
    "  else:\n",
    "    triplet_sum = 0\n",
    "  if triplet_sum == 3:\n",
    "    triplet_sum = 0\n",
    "  current_beat, current_measure_idx, measure_changed = self.update_condition_info(current_beat, current_measure_idx, measure_duration, current_dur)\n",
    "  if 'measure_idx' in self.tokenizer.tok2idx and current_measure_idx > self.tokenizer.vocab['measure_idx'][-1]:\n",
    "    break\n",
    "  if float(current_beat) == 10.0:\n",
    "    print(f\"Current Beat ==10.0 detected! cur_beat: {current_beat}, measure_dur: {measure_duration}, cur_measure_idx: {current_measure_idx}, cur_dur: {current_dur}, triplet_sum: {triplet_sum}\")\n",
    "  condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "  # make new token for next rnn timestep\n",
    "  selected_token = torch.cat([torch.LongTensor([[part_idx]]).to(dev), selected_token, torch.LongTensor([condition_tokens]).to(dev)], dim=1)\n",
    "  \n",
    "  final_tokens.append(selected_token)\n",
    "  \n",
    "if len(total_attention_weights) == 0:\n",
    "  attention_map = None\n",
    "else:\n",
    "  attention_map = torch.stack(total_attention_weights, dim=1)\n",
    "  \n",
    "cat_out = torch.cat(final_tokens[1:], dim=0) #token shape: [1,6] => [24,6]\n",
    "newly_generated = cat_out.clone()\n",
    "if prev_generation is not None:\n",
    "  cat_out = torch.cat([prev_generation[1:], cat_out], dim=0)\n",
    "    #  self.converter(src[1:-1]), self.converter(torch.cat(final_tokens, dim=0)), attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 35])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 35])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prob.shape)\n",
    "prob[:, -1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9818e-04, 3.7798e-04, 4.8585e-04, 2.7706e-04, 2.2031e-03, 9.2545e-01,\n",
       "        1.2181e-04, 1.1466e-03, 1.6351e-03, 1.7138e-03, 1.0574e-03, 4.8105e-05,\n",
       "        6.2154e-02, 1.7685e-03, 5.6734e-04, 8.1531e-05, 1.7565e-05, 3.5282e-05,\n",
       "        5.6226e-04, 8.6134e-04, 8.0524e-04, 1.5512e-03, 9.1624e-04, 7.2504e-01,\n",
       "        1.1391e-04, 2.6486e-03, 5.9353e-04, 5.7281e-03, 7.9037e-04, 1.1007e-03,\n",
       "        6.9005e-03, 6.7596e-04, 2.3594e-03, 1.0879e-03, 2.4882e-01],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 8],\n",
       "        [5, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.decoder._select_token(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_inference(self, src, part_idx, prev_generation=None, fix_first_beat=False, compensate_beat=(0.0, 0.0)):\n",
    "    dev = src.device\n",
    "    assert src.ndim == 2\n",
    "    enc_out, enc_mask = self.encoder(src.unsqueeze(0)) \n",
    "\n",
    "    # Setup for 0th step\n",
    "    # start_token = torch.LongTensor([[part_idx, 1, 1, 3, 3, 4]]) # start token idx is 1\n",
    "    start_token = self.get_start_token(part_idx).to(dev)\n",
    "  \n",
    "    #-------------\n",
    "    # not_triplet_vocab_list = [ i for i, dur in enumerate(self.tokenizer.vocab['duration'][3:], 3) if (dur%0.5) != 0.0]\n",
    "    #-------------\n",
    "    measure_duration = self._get_measure_duration(part_idx)\n",
    "    current_measure_idx = 0\n",
    "    measure_changed = 1\n",
    "\n",
    "    final_tokens = [start_token]\n",
    "    if prev_generation is not None:\n",
    "      final_tokens, current_measure_idx, last_hidden = self._apply_prev_generation(prev_generation, final_tokens, last_hidden, enc_out)\n",
    "      # emb = self.decoder._get_embedding(prev_generation)\n",
    "      # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "      # start_token = torch.cat([prev_generation[-1:, :3].to(dev),  torch.LongTensor([[3, 3, 5]]).to(dev)], dim=-1)\n",
    "      # final_tokens.append(start_token)\n",
    "      # current_measure_idx = 2\n",
    "    selected_token = start_token\n",
    "    \n",
    "    current_beat = Fraction(0, 1)\n",
    "\n",
    "    total_attention_weights = []\n",
    "    # while True:\n",
    "    triplet_sum = 0 \n",
    "    condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "\n",
    "    for i in range(300):\n",
    "      # decode_out, last_hidden = self.decoder.rnn(emb.unsqueeze(0), last_hidden)\n",
    "      # attention_vectors, attention_weight = self._get_attention_vector(encode_out, decode_out)\n",
    "      # combined_value = torch.cat([decode_out, attention_vectors], dim=-1)\n",
    "      # combined_value, last_hidden, attention_weight = self._run_inference_on_step(emb, last_hidden, encode_out)\n",
    "      prob = self.decoder(torch.cat(final_tokens, dim=0).unsqueeze(0), enc_out, enc_mask)\n",
    "      selected_token = self.decoder._select_token(prob)\n",
    "      \n",
    "      # for rule_base fixing\n",
    "      if fix_first_beat and current_beat - compensate_beat[1] == 0.0:\n",
    "        for src_token in src:\n",
    "          if src_token[-1] == condition_tokens[-1] and self.tokenizer.vocab['offset'][src_token[3].item()] - compensate_beat[0] == 0.0:\n",
    "            # if measure_idx is same and offset is same\n",
    "            selected_token[0, 0] = src_token[1] # get pitch from src\n",
    "            break\n",
    "      \n",
    "      if selected_token[0, 0]==self.tokenizer.tok2idx['pitch']['end'] or selected_token[0, 1]==self.tokenizer.tok2idx['duration']['end']:\n",
    "        break\n",
    "      \n",
    "      # update beat position and beat strength\n",
    "      current_dur = self.tokenizer.vocab['duration'][selected_token[0, 1]]\n",
    "      if Fraction(current_dur).limit_denominator(3).denominator == 3:\n",
    "        triplet_sum += Fraction(current_dur).limit_denominator(3).numerator\n",
    "      elif triplet_sum != 0:\n",
    "        # print(\"Triplet has to appear but not\", current_dur,)\n",
    "        triplet_sum += 1\n",
    "        # print(current_dur, selected_token)\n",
    "        current_dur = Fraction(1,3)\n",
    "        selected_token = torch.LongTensor([[selected_token[0,0], self.tokenizer.tok2idx['duration'][current_dur]]]).to(dev)\n",
    "        # print(current_dur, selected_token)\n",
    "      else:\n",
    "        triplet_sum = 0\n",
    "      if triplet_sum == 3:\n",
    "        triplet_sum = 0\n",
    "      current_beat, current_measure_idx, measure_changed = self.update_condition_info(current_beat, current_measure_idx, measure_duration, current_dur)\n",
    "      if 'measure_idx' in self.tokenizer.tok2idx and current_measure_idx > self.tokenizer.vocab['measure_idx'][-1]:\n",
    "        break\n",
    "      if float(current_beat) == 10.0:\n",
    "        print(f\"Current Beat ==10.0 detected! cur_beat: {current_beat}, measure_dur: {measure_duration}, cur_measure_idx: {current_measure_idx}, cur_dur: {current_dur}, triplet_sum: {triplet_sum}\")\n",
    "      condition_tokens = self.encode_condition_token(part_idx, current_beat, current_measure_idx, measure_changed)\n",
    "      \n",
    "      # make new token for next rnn timestep\n",
    "      selected_token = torch.cat([torch.LongTensor([[part_idx]]).to(dev), selected_token, torch.LongTensor([condition_tokens]).to(dev)], dim=1)\n",
    "      \n",
    "      final_tokens.append(selected_token)\n",
    "      \n",
    "    if len(total_attention_weights) == 0:\n",
    "      attention_map = None\n",
    "    else:\n",
    "      attention_map = torch.stack(total_attention_weights, dim=1)\n",
    "      \n",
    "    cat_out = torch.cat(final_tokens[1:], dim=0) #token shape: [1,6] => [24,6]\n",
    "    newly_generated = cat_out.clone()\n",
    "    if prev_generation is not None:\n",
    "      cat_out = torch.cat([prev_generation[1:], cat_out], dim=0)\n",
    "    #  self.converter(src[1:-1]), self.converter(torch.cat(final_tokens, dim=0)), attention_map\n",
    "\n",
    "    return self._decode_inference_result(src, cat_out, (attention_map, cat_out, newly_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))\n\u001b[1;32m      2\u001b[0m out_a \u001b[38;5;241m=\u001b[39m encoder(batch[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m modified_x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "out_a = encoder(batch[0])\n",
    "\n",
    "modified_x = batch[0].clone()\n",
    "modified_x[0, 4, 1:] = 3\n",
    "out_b = encoder(modified_x)\n",
    "\n",
    "out_a == out_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (240) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mx: (batch, seq_len, num_features)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# squeeze num_features dimension\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/x_transformers.py:1334\u001b[0m, in \u001b[0;36mAttentionLayers.forward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, self_attn_kv_mask, mems, mem_masks, seq_start_pos, cache, cache_age, return_hiddens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         layer_mem \u001b[38;5;241m=\u001b[39m pre_norm(layer_mem)\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1334\u001b[0m     out, inter \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mself_attn_kv_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_attn_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer_mem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer_mem_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_intermediates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m layer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1336\u001b[0m     out, inter \u001b[38;5;241m=\u001b[39m block(x, context \u001b[38;5;241m=\u001b[39m context, mask \u001b[38;5;241m=\u001b[39m mask, context_mask \u001b[38;5;241m=\u001b[39m context_mask, prev_attn \u001b[38;5;241m=\u001b[39m prev_cross_attn, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_attn_cache, \u001b[38;5;28;01mNone\u001b[39;00m), return_intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/x_transformers.py:944\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, rel_pos, rotary_pos_emb, prev_attn, mem, mem_mask, return_intermediates, cache)\u001b[0m\n\u001b[1;32m    940\u001b[0m     attn_bias \u001b[38;5;241m=\u001b[39m rel_pos(i, j)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# attention is all we need\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m out, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinal_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_attn\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# https://arxiv.org/abs/2208.06061 proposes to add a residual for better gradients\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(r):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SejongMusic-0Xd-sAL0/lib/python3.8/site-packages/x_transformers/attend.py:300\u001b[0m, in \u001b[0;36mAttend.forward\u001b[0;34m(self, q, k, v, mask, attn_bias, prev_attn)\u001b[0m\n\u001b[1;32m    297\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m&\u001b[39m sparse_topk_mask) \u001b[38;5;28;01mif\u001b[39;00m exists(mask) \u001b[38;5;28;01melse\u001b[39;00m sparse_topk_mask\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(mask):\n\u001b[0;32m--> 300\u001b[0m     dots \u001b[38;5;241m=\u001b[39m \u001b[43mdots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m causal:\n\u001b[1;32m    303\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_causal_mask(i, j, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (240) at non-singleton dimension 3"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
